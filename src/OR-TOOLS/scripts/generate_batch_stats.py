import os
import sys
import subprocess
import re
import json
from pathlib import Path

# Add parent directories to sys.path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

STATS_SCRIPT = PROJECT_ROOT / "src" / "analysis" / "generate_statistics.py"

def parse_scores_from_log(log_file):
    """
    Extrait les scores d'optimisation depuis un fichier de log.
    
    Cherche des lignes comme:
    Score brut : 155,290,666,940
    Score maximum th√©orique : 290,808,782,540
    Score normalis√© : 53.40/100
    
    Returns:
        dict ou None si les scores ne sont pas trouv√©s
    """
    if not log_file.exists():
        return None
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Patterns pour extraire les scores
        # Score brut peut avoir des virgules comme s√©parateurs de milliers
        raw_pattern = r"Score brut\s*:\s*([\d,]+)"
        max_pattern = r"Score maximum th√©orique\s*:\s*([\d,]+)"
        norm_pattern = r"Score normalis√©\s*:\s*([\d.]+)/100"
        
        raw_match = re.search(raw_pattern, content)
        max_match = re.search(max_pattern, content)
        norm_match = re.search(norm_pattern, content)
        
        if raw_match and max_match and norm_match:
            # Nettoyer les virgules des nombres
            raw_score = int(raw_match.group(1).replace(',', ''))
            max_score = int(max_match.group(1).replace(',', ''))
            norm_score = float(norm_match.group(1))
            
            # D√©tecter le statut (OPTIMAL ou FEASIBLE)
            status = "FEASIBLE"
            if "OPTIMAL" in content:
                status = "OPTIMAL"
            
            return {
                "raw_score": raw_score,
                "max_theoretical_score": max_score,
                "normalized_score": norm_score,
                "status": status
            }
        else:
            return None
            
    except Exception as e:
        print(f"  ‚ö† Erreur lors du parsing du log : {e}")
        return None

def generate_stats_for_iterations(base_folder):
    """
    Parcourt tous les dossiers d'it√©rations et g√©n√®re les statistiques
    pour chaque fichier CSV trouv√©.
    
    Args:
        base_folder: Dossier de base contenant les sous-dossiers (ex: T1200/V5_01/)
    """
    base_path = Path(base_folder)
    
    if not base_path.exists():
        print(f"Erreur: Le dossier {base_folder} n'existe pas.")
        return
    
    # Chercher le dossier iters/
    iters_folder = base_path / "iters"
    stats_folder = base_path / "stats"
    logs_folder = base_path / "logs"
    
    if not iters_folder.exists():
        print(f"Erreur: Le dossier {iters_folder} n'existe pas.")
        return
    
    # Cr√©er le dossier stats s'il n'existe pas
    stats_folder.mkdir(parents=True, exist_ok=True)
    
    # Trouver tous les fichiers CSV
    csv_files = sorted(iters_folder.glob("*.csv"))
    
    if not csv_files:
        print(f"Aucun fichier CSV trouv√© dans {iters_folder}")
        return
    
    print(f"Trouv√© {len(csv_files)} fichiers CSV dans {iters_folder}")
    print(f"G√©n√©ration des statistiques dans {stats_folder}")
    print("=" * 70)
    
    success_count = 0
    failed_count = 0
    
    for csv_file in csv_files:
        # G√©n√©rer le nom du fichier de sortie
        base_name = csv_file.stem  # nom sans extension
        stats_file = stats_folder / f"stats_{base_name}.xlsx"
        
        # Chercher le fichier log correspondant
        log_file = logs_folder / f"log_{base_name}.txt"
        
        print(f"\n[{success_count + failed_count + 1}/{len(csv_files)}] {csv_file.name}")
        print(f"  ‚Üí CSV: {csv_file}")
        print(f"  ‚Üí Stats: {stats_file.name}")
        
        # Parser les scores depuis le log
        optimization_scores = None
        if log_file.exists():
            print(f"  ‚Üí Log: {log_file.name}")
            optimization_scores = parse_scores_from_log(log_file)
            if optimization_scores:
                print(f"  ‚úì Scores extraits du log: {optimization_scores['normalized_score']:.2f}/100")
            else:
                print(f"  ‚ö† Impossible d'extraire les scores du log")
        else:
            print(f"  ‚ö† Pas de fichier log trouv√©")
        
        # Cr√©er un fichier JSON temporaire avec les scores si disponibles
        temp_json = None
        if optimization_scores:
            temp_json = csv_file.parent / f"{base_name}_scores.json"
            try:
                with open(temp_json, 'w', encoding='utf-8') as f:
                    json.dump(optimization_scores, f, indent=2)
            except Exception as e:
                print(f"  ‚ö† Erreur lors de la cr√©ation du JSON temporaire : {e}")
                temp_json = None
        
        try:
            # Construire la commande
            cmd = [
                sys.executable,
                str(STATS_SCRIPT),
                "--input", str(csv_file),
                "--output", str(stats_file)
            ]
            
            # Ex√©cuter le script de statistiques
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                cwd=str(PROJECT_ROOT)
            )
            
            print(f"  ‚úì Statistiques g√©n√©r√©es avec succ√®s")
            success_count += 1
            
            # Nettoyer le fichier JSON temporaire
            if temp_json and temp_json.exists():
                try:
                    temp_json.unlink()
                except:
                    pass
            
        except subprocess.CalledProcessError as e:
            print(f"  ‚úó ERREUR lors de la g√©n√©ration des statistiques")
            print(f"    {e.stderr if e.stderr else e.stdout}")
            failed_count += 1
            
            # Nettoyer le fichier JSON temporaire en cas d'erreur aussi
            if temp_json and temp_json.exists():
                try:
                    temp_json.unlink()
                except:
                    pass
                    
        except Exception as e:
            print(f"  ‚úó ERREUR: {str(e)}")
            failed_count += 1
            
            # Nettoyer le fichier JSON temporaire en cas d'erreur aussi
            if temp_json and temp_json.exists():
                try:
                    temp_json.unlink()
                except:
                    pass
    
    print("\n" + "=" * 70)
    print(f"Termin√©: {success_count} r√©ussis, {failed_count} √©chou√©s")
    print(f"Statistiques sauvegard√©es dans: {stats_folder}")
    print("=" * 70)
    
    # G√©n√©rer la synth√®se des scores
    generate_scores_summary(stats_folder, base_path)

def generate_scores_summary(stats_folder, base_path):
    """
    G√©n√®re un fichier de synth√®se avec les statistiques des scores (moyenne, min, max, √©cart-type).
    
    Args:
        stats_folder: Dossier contenant les fichiers JSON de scores
        base_path: Dossier de base pour d√©terminer le nom de configuration
    """
    import statistics
    
    # Collecter tous les fichiers JSON de scores
    json_files = sorted(stats_folder.glob("*_scores.json"))
    
    if not json_files:
        print("\n  ‚ö† Aucun fichier de scores trouv√© pour la synth√®se")
        return
    
    # Collecter les scores
    scores_data = {
        "raw_scores": [],
        "max_theoretical_scores": [],
        "normalized_scores": [],
        "statuses": []
    }
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                scores_data["raw_scores"].append(data["raw_score"])
                scores_data["max_theoretical_scores"].append(data["max_theoretical_score"])
                scores_data["normalized_scores"].append(data["normalized_score"])
                scores_data["statuses"].append(data["status"])
        except Exception as e:
            print(f"  ‚ö† Erreur lors de la lecture de {json_file.name}: {e}")
            continue
    
    if not scores_data["raw_scores"]:
        print("\n  ‚ö† Aucun score valide trouv√©")
        return
    
    # Calculer les statistiques
    n = len(scores_data["raw_scores"])
    
    summary = {
        "configuration": {
            "model": base_path.name,  # V5_01, V5_02, etc.
            "time_limit": base_path.parent.name,  # T1200, T1800, etc.
            "date": base_path.parent.parent.name,  # Date folder
            "iterations_count": n
        },
        "raw_score": {
            "mean": round(statistics.mean(scores_data["raw_scores"]), 2),
            "min": min(scores_data["raw_scores"]),
            "max": max(scores_data["raw_scores"]),
            "stdev": round(statistics.stdev(scores_data["raw_scores"]), 2) if n > 1 else 0
        },
        "max_theoretical_score": {
            "mean": round(statistics.mean(scores_data["max_theoretical_scores"]), 2),
            "min": min(scores_data["max_theoretical_scores"]),
            "max": max(scores_data["max_theoretical_scores"])
        },
        "normalized_score": {
            "mean": round(statistics.mean(scores_data["normalized_scores"]), 2),
            "min": round(min(scores_data["normalized_scores"]), 2),
            "max": round(max(scores_data["normalized_scores"]), 2),
            "stdev": round(statistics.stdev(scores_data["normalized_scores"]), 2) if n > 1 else 0
        },
        "status": {
            "OPTIMAL": scores_data["statuses"].count("OPTIMAL"),
            "FEASIBLE": scores_data["statuses"].count("FEASIBLE")
        }
    }
    
    # Sauvegarder le fichier de synth√®se
    summary_file = stats_folder / "scores_summary.json"
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"\n  ‚úì Synth√®se des scores sauvegard√©e: {summary_file.name}")
        
        # Afficher un r√©sum√©
        print(f"\n  üìä R√©sum√© des scores ({n} it√©rations):")
        print(f"     Score normalis√© moyen: {summary['normalized_score']['mean']:.2f}/100")
        print(f"     Min: {summary['normalized_score']['min']:.2f}, Max: {summary['normalized_score']['max']:.2f}")
        print(f"     √âcart-type: {summary['normalized_score']['stdev']:.2f}")
        print(f"     Solutions OPTIMAL: {summary['status']['OPTIMAL']}, FEASIBLE: {summary['status']['FEASIBLE']}")
        
    except Exception as e:
        print(f"\n  ‚úó Erreur lors de la sauvegarde de la synth√®se: {e}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="G√©n√®re les statistiques pour tous les fichiers CSV d'it√©rations"
    )
    parser.add_argument(
        "folder",
        type=str,
        help="Chemin vers le dossier contenant les sous-dossiers iters/ et stats/ (ex: batch_experiments/04_02_2026/T1200/V5_01)"
    )
    
    args = parser.parse_args()
    
    generate_stats_for_iterations(args.folder)

if __name__ == "__main__":
    main()
